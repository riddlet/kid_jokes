{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pytumblr\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import AppAuthHandler\n",
    "from tweepy import Stream\n",
    "import urllib2\n",
    "import time\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect tumblr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = pytumblr.TumblrRestClient(\n",
    "  'XXX',\n",
    "  'XXX',\n",
    "  'XXX',\n",
    "  'XXX'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posts = client.posts('badkidsjokes')\n",
    "total_posts = range(posts['total_posts'])\n",
    "for i in total_posts[20::20]:\n",
    "    page = client.posts('badkidsjokes', offset=i)\n",
    "    posts['posts'].extend(page['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tumblrdat.json', 'w') as f:\n",
    "    json.dump(posts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_token = \"XXX\"\n",
    "access_token_secret = \"XXX\"\n",
    "consumer_key = \"XXX\"\n",
    "consumer_secret = \"XXX\"\n",
    "\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alltweets = []\n",
    "new_tweets = api.user_timeline(screen_name='KidsWriteJokes', count=200)\n",
    "alltweets.extend(new_tweets)\n",
    "if len(new_tweets)>0:\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    while len(new_tweets) > 0:\n",
    "        new_tweets=api.user_timeline(screen_name='KidsWriteJokes', count=200, max_id=oldest)\n",
    "        alltweets.extend(new_tweets)\n",
    "        oldest = alltweets[-1].id-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tweets = api.user_timeline(screen_name='reallybadlaughs', count=200)\n",
    "alltweets.extend(new_tweets)\n",
    "if len(new_tweets)>0:\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    while len(new_tweets) > 0:\n",
    "        new_tweets=api.user_timeline(screen_name='reallybadlaughs', count=200, max_id=oldest)\n",
    "        alltweets.extend(new_tweets)\n",
    "        oldest = alltweets[-1].id-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "679"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alltweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('twitterdat.json', 'w') as f:\n",
    "    json.dump([tweet._json for tweet in alltweets], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request_until_succeed(url):\n",
    "    \"\"\"\n",
    "    handles potential errors when sending a request\n",
    "    \"\"\"\n",
    "    \n",
    "    req = urllib2.Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try: \n",
    "            response = urllib2.urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception, e:\n",
    "            print e\n",
    "            time.sleep(5)\n",
    "\n",
    "    return response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "app_id = 'XXX'\n",
    "app_secret = 'XXX'\n",
    "baseurl = 'https://graph.facebook.com/v2.8/kidswritejokes/feed/?fields=message'\n",
    "access_token = '&access_token=' + app_id + '|' + app_secret\n",
    "url = baseurl+access_token\n",
    "dat = json.loads(request_until_succeed(url))\n",
    "items=dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "has_next_page = True\n",
    "while has_next_page:\n",
    "    if 'paging' not in dat.keys():\n",
    "        break\n",
    "    \n",
    "    dat = json.loads(request_until_succeed(dat['paging']['next']))\n",
    "    items['data'].extend(dat['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('facebookdat.json', 'w') as f:\n",
    "    json.dump(items, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open and combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('facebookdat.json') as f:\n",
    "    fb_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('twitterdat.json') as f:\n",
    "    twitter_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tumblrdat.json') as f:\n",
    "    tumblr_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jokes = []\n",
    "for i in fb_data['data']:\n",
    "    jokes.append(i['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in tumblr_data['posts']:\n",
    "    jokes.append(i['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in twitter_data:\n",
    "    jokes.append(i['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jokes = [BeautifulSoup(j, 'lxml').get_text() for j in jokes]\n",
    "#[j.lower() for j in jokes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Corpus Length:', 123297)\n"
     ]
    }
   ],
   "source": [
    "CORPUS_LENGTH = None\n",
    "\n",
    "def get_corpus(joke_dat, verbose=0):\n",
    "    emoji_pattern = re.compile(\n",
    "        u\"(\\ud83d[\\ude00-\\ude4f])|\"  # emoticons\n",
    "        u\"(\\ud83c[\\udf00-\\uffff])|\"  # symbols & pictographs (1 of 2)\n",
    "        u\"(\\ud83d[\\u0000-\\uddff])|\"  # symbols & pictographs (2 of 2)\n",
    "        u\"(\\ud83d[\\ude80-\\udeff])|\"  # transport & map symbols\n",
    "        u\"(\\ud83c[\\udde0-\\uddff])\"  # flags (iOS)\n",
    "        u'\\U0001F300-\\U0001F64F'\n",
    "        u'\\U0001F680-\\U0001F6FF'\n",
    "        u'\\u2600-\\u26FF\\u2700-\\u27BF]+', \n",
    "        flags=re.UNICODE)\n",
    "    joke_dat = [t for t in joke_dat if 'http' not in t]\n",
    "    joke_dat = [t for t in joke_dat if '@' not in t]\n",
    "    joke_dat = [emoji_pattern.sub(r'', t) for t in joke_dat]\n",
    "    corpus = u' '.join(joke_dat)\n",
    "    global CORPUS_LENGTH\n",
    "    CORPUS_LENGTH = len(corpus)\n",
    "    if verbose:\n",
    "        print('Corpus Length:', CORPUS_LENGTH)\n",
    "    return corpus\n",
    "\n",
    "corpus = get_corpus(jokes, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('No. of unique characters:', 93)\n"
     ]
    }
   ],
   "source": [
    "N_CHARS = None\n",
    "\n",
    "def create_index_char_map(corpus, verbose=0):\n",
    "    chars = sorted(list(set(corpus)))\n",
    "    global N_CHARS\n",
    "    N_CHARS = len(chars)\n",
    "    if verbose:\n",
    "        print('No. of unique characters:', N_CHARS)\n",
    "    char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "    idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char\n",
    "\n",
    "chars, char_to_idx, idx_to_char = create_index_char_map(corpus, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('No. of sequences:', 41086)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 40\n",
    "SEQ_STEP = 3\n",
    "N_SEQS = None\n",
    "\n",
    "def create_sequences(corpus, verbose=0):\n",
    "    sequences, next_chars = [], []\n",
    "    for i in range(0, CORPUS_LENGTH - MAX_SEQ_LENGTH, SEQ_STEP):\n",
    "        sequences.append(corpus[i:i + MAX_SEQ_LENGTH])\n",
    "        next_chars.append(corpus[i + MAX_SEQ_LENGTH])\n",
    "    global N_SEQS\n",
    "    N_SEQS = len(sequences)\n",
    "    if verbose:\n",
    "        print('No. of sequences:', len(sequences))\n",
    "    return np.array(sequences), np.array(next_chars)\n",
    "\n",
    "sequences, next_chars = create_sequences(corpus, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(sequences, next_chars, char_to_idx):\n",
    "    X = np.zeros((N_SEQS, MAX_SEQ_LENGTH, N_CHARS), dtype=np.bool)\n",
    "    y = np.zeros((N_SEQS, N_CHARS), dtype=np.bool)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for t, char in enumerate(sequence):\n",
    "            X[i, t, char_to_idx[char]] = 1\n",
    "            y[i, char_to_idx[next_chars[i]]] = 1\n",
    "    return X, y\n",
    "\n",
    "X, y = one_hot_encode(sequences, next_chars, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_5 (LSTM)                    (None, 40, 128)       113664      lstm_input_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 40, 128)       0           lstm_5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                    (None, 128)           131584      dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 128)           0           lstm_6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 93)            11997       dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 257,245\n",
      "Trainable params: 257,245\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(hidden_layer_size=128, dropout=0.2, learning_rate=0.01, verbose=0):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, input_shape=(MAX_SEQ_LENGTH, N_CHARS)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(N_CHARS, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=learning_rate))\n",
    "    if verbose:\n",
    "        print('Model Summary:')\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "model = build_model(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.9990Epoch 00000: loss improved from inf to 1.99892, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 126s - loss: 1.9989   \n",
      "Epoch 2/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.8872Epoch 00001: loss improved from 1.99892 to 1.88650, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 126s - loss: 1.8865   \n",
      "Epoch 3/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.8072Epoch 00002: loss improved from 1.88650 to 1.80745, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 125s - loss: 1.8074   \n",
      "Epoch 4/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.7363Epoch 00003: loss improved from 1.80745 to 1.73648, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 126s - loss: 1.7365   \n",
      "Epoch 5/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.6881Epoch 00004: loss improved from 1.73648 to 1.68839, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 127s - loss: 1.6884   \n",
      "Epoch 6/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.6431Epoch 00005: loss improved from 1.68839 to 1.64301, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 127s - loss: 1.6430   \n",
      "Epoch 7/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.6079Epoch 00006: loss improved from 1.64301 to 1.60819, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 127s - loss: 1.6082   \n",
      "Epoch 8/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.5738Epoch 00007: loss improved from 1.60819 to 1.57353, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 127s - loss: 1.5735   \n",
      "Epoch 9/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.5423Epoch 00008: loss improved from 1.57353 to 1.54255, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.5425   \n",
      "Epoch 10/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.5194Epoch 00009: loss improved from 1.54255 to 1.51908, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 127s - loss: 1.5191   \n",
      "Epoch 11/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.4951Epoch 00010: loss improved from 1.51908 to 1.49498, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.4950   \n",
      "Epoch 12/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.4754Epoch 00011: loss improved from 1.49498 to 1.47537, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.4754   \n",
      "Epoch 13/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.4531Epoch 00012: loss improved from 1.47537 to 1.45319, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.4532   \n",
      "Epoch 14/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.4446Epoch 00013: loss improved from 1.45319 to 1.44489, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.4449   \n",
      "Epoch 15/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.4188Epoch 00014: loss improved from 1.44489 to 1.41837, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.4184   \n",
      "Epoch 16/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.4018Epoch 00015: loss improved from 1.41837 to 1.40163, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.4016   \n",
      "Epoch 17/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.3902Epoch 00016: loss improved from 1.40163 to 1.39000, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.3900   \n",
      "Epoch 18/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.3811Epoch 00017: loss improved from 1.39000 to 1.38159, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.3816   \n",
      "Epoch 19/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.3717Epoch 00018: loss improved from 1.38159 to 1.37111, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.3711   \n",
      "Epoch 20/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.3537Epoch 00019: loss improved from 1.37111 to 1.35393, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.3539   \n",
      "Epoch 21/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.3506Epoch 00020: loss improved from 1.35393 to 1.35123, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.3512   \n",
      "Epoch 22/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.3423Epoch 00021: loss improved from 1.35123 to 1.34254, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.3425   \n",
      "Epoch 23/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.3278Epoch 00022: loss improved from 1.34254 to 1.32847, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.3285   \n",
      "Epoch 24/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.3211Epoch 00023: loss improved from 1.32847 to 1.32128, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 129s - loss: 1.3213   \n",
      "Epoch 25/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.3163Epoch 00024: loss improved from 1.32128 to 1.31621, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.3162   \n",
      "Epoch 26/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2997Epoch 00025: loss improved from 1.31621 to 1.30005, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.3001   \n",
      "Epoch 27/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2873Epoch 00026: loss improved from 1.30005 to 1.28796, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.2880   \n",
      "Epoch 28/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2830Epoch 00027: loss improved from 1.28796 to 1.28356, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.2836   \n",
      "Epoch 29/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2789Epoch 00028: loss improved from 1.28356 to 1.27829, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.2783   \n",
      "Epoch 30/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2674Epoch 00029: loss improved from 1.27829 to 1.26769, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.2677   \n",
      "Epoch 31/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2599Epoch 00030: loss improved from 1.26769 to 1.26032, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.2603   \n",
      "Epoch 32/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2512Epoch 00031: loss improved from 1.26032 to 1.25056, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 127s - loss: 1.2506   \n",
      "Epoch 33/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2490Epoch 00032: loss improved from 1.25056 to 1.25013, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 129s - loss: 1.2501   \n",
      "Epoch 34/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2291Epoch 00033: loss improved from 1.25013 to 1.22930, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.2293   \n",
      "Epoch 35/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2397Epoch 00034: loss did not improve\n",
      "41086/41086 [==============================] - 128s - loss: 1.2398   \n",
      "Epoch 36/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2303Epoch 00035: loss did not improve\n",
      "41086/41086 [==============================] - 128s - loss: 1.2312   \n",
      "Epoch 37/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2262Epoch 00036: loss improved from 1.22930 to 1.22691, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 130s - loss: 1.2269   \n",
      "Epoch 38/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2182Epoch 00037: loss improved from 1.22691 to 1.21899, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.2190   \n",
      "Epoch 39/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2296Epoch 00038: loss did not improve\n",
      "41086/41086 [==============================] - 128s - loss: 1.2293   \n",
      "Epoch 40/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2207Epoch 00039: loss did not improve\n",
      "41086/41086 [==============================] - 129s - loss: 1.2205   \n",
      "Epoch 41/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2163Epoch 00040: loss improved from 1.21899 to 1.21653, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 129s - loss: 1.2165   \n",
      "Epoch 42/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1988Epoch 00041: loss improved from 1.21653 to 1.19912, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.1991   \n",
      "Epoch 43/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.2017Epoch 00042: loss did not improve\n",
      "41086/41086 [==============================] - 128s - loss: 1.2019   \n",
      "Epoch 44/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1874Epoch 00043: loss improved from 1.19912 to 1.18755, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 129s - loss: 1.1876   \n",
      "Epoch 45/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1986Epoch 00044: loss did not improve\n",
      "41086/41086 [==============================] - 128s - loss: 1.1980   \n",
      "Epoch 46/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1938Epoch 00045: loss did not improve\n",
      "41086/41086 [==============================] - 128s - loss: 1.1944   \n",
      "Epoch 47/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1806Epoch 00046: loss improved from 1.18755 to 1.17997, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 129s - loss: 1.1800   \n",
      "Epoch 48/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1775Epoch 00047: loss improved from 1.17997 to 1.17705, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.1770   \n",
      "Epoch 49/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1709Epoch 00048: loss improved from 1.17705 to 1.17109, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 129s - loss: 1.1711   \n",
      "Epoch 50/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1729Epoch 00049: loss did not improve\n",
      "41086/41086 [==============================] - 128s - loss: 1.1731   \n",
      "Epoch 51/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1525Epoch 00050: loss improved from 1.17109 to 1.15347, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.1535   \n",
      "Epoch 52/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1589Epoch 00051: loss did not improve\n",
      "41086/41086 [==============================] - 128s - loss: 1.1593   \n",
      "Epoch 53/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1594Epoch 00052: loss did not improve\n",
      "41086/41086 [==============================] - 128s - loss: 1.1591   \n",
      "Epoch 54/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1502Epoch 00053: loss improved from 1.15347 to 1.14997, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.1500   \n",
      "Epoch 55/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1423Epoch 00054: loss improved from 1.14997 to 1.14247, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.1425   \n",
      "Epoch 56/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1475Epoch 00055: loss did not improve\n",
      "41086/41086 [==============================] - 128s - loss: 1.1474   \n",
      "Epoch 57/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1406Epoch 00056: loss improved from 1.14247 to 1.14115, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.1412   \n",
      "Epoch 58/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1342Epoch 00057: loss improved from 1.14115 to 1.13370, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 129s - loss: 1.1337   \n",
      "Epoch 59/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1325Epoch 00058: loss improved from 1.13370 to 1.13270, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.1327   \n",
      "Epoch 60/60\n",
      "40960/41086 [============================>.] - ETA: 0s - loss: 1.1274Epoch 00059: loss improved from 1.13270 to 1.12729, saving model to weights_seq40_step3.hdf5\n",
      "41086/41086 [==============================] - 128s - loss: 1.1273   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(model, X, y, batch_size=128, nb_epoch=60, verbose=0):\n",
    "    checkpointer = ModelCheckpoint(filepath=\"weights_seq40_step3.hdf5\", monitor='loss', verbose=verbose, save_best_only=True, mode='min')\n",
    "    model.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose, callbacks=[checkpointer])\n",
    "\n",
    "train_model(model, X, y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception RuntimeError: RuntimeError('Failed to retrieve old handler',) in 'h5py._errors.set_error_handler' ignored\n",
      "Exception RuntimeError: RuntimeError('Failed to retrieve old handler',) in 'h5py._errors.set_error_handler' ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet no. 001\n",
      "=============\n",
      "Generating with seed:\n",
      "what did the women do in bed when she he\n",
      "________________________________________\n",
      "what did the women do in bed when she hear and a banana with no eyes\n",
      "\n",
      "because it was a boy and talk and said to get the sents a boy and a bo\n",
      "()\n",
      "Tweet no. 002\n",
      "=============\n",
      "Generating with seed:\n",
      "why did the zebra wear stripy pijamas\n",
      "be\n",
      "________________________________________\n",
      "why did the zebra wear stripy pijamas\n",
      "because they don’t have a fish with no eye\n",
      "\n",
      "\n",
      "a cheese was a boy and has a boy and the cow and said to \n",
      "()\n",
      "Tweet no. 003\n",
      "=============\n",
      "Generating with seed:\n",
      "Why did they let the turkey join the ban\n",
      "________________________________________\n",
      "Why did they let the turkey join the banana fart and a boy with a man and a boy and talk a baby poo in a banana and a boy and a boy and a pi\n",
      "()\n",
      "Tweet no. 004\n",
      "=============\n",
      "Generating with seed:\n",
      "what do cavemen eat\n",
      "\n",
      "caves\n",
      "__________________________\n",
      "what do cavemen eat\n",
      "\n",
      "cavesIYYYIIYYYYYYSYYYYYYYYYIYYYIYIIYSYYIYYYYYYIYYYYYYYIYYYYYIYYYYYYYYYYYYYYYYYYYYYYYYYIYYYYIYYYYYYYYYYYYY\n",
      "()\n",
      "Tweet no. 005\n",
      "=============\n",
      "Generating with seed:\n",
      "hi \n",
      "ok i ask you out \n",
      "but i haet you \n",
      "bu\n",
      "________________________________________\n",
      "hi \n",
      "ok i ask you out \n",
      "but i haet you \n",
      "but your butt\n",
      "but a cheese was a boy and talk of frum a hatman and a boy and has a hants to go to the \n",
      "()\n",
      "Tweet no. 006\n",
      "=============\n",
      "Generating with seed:\n",
      "Why cant the T-Rex clap its hands? Becau\n",
      "________________________________________\n",
      "Why cant the T-Rex clap its hands? Because they dont say to the school who do you call a cow with a man with a hants what do you call a cow \n",
      "()\n",
      "Tweet no. 007\n",
      "=============\n",
      "Generating with seed:\n",
      "What is pink, white and sits in corners \n",
      "________________________________________\n",
      "What is pink, white and sits in corners what do you call a man with a man a ha for a poo hard on the chicken land on the mother?\n",
      "\n",
      "A car.. wh\n",
      "()\n",
      "Tweet no. 008\n",
      "=============\n",
      "Generating with seed:\n",
      "A duck walks into a bar. A man runs out \n",
      "________________________________________\n",
      "A duck walks into a bar. A man runs out of my house and a boy and has a hant and a boy and talk and a boy and talk and a building with a har\n",
      "()\n",
      "Tweet no. 009\n",
      "=============\n",
      "Generating with seed:\n",
      "what do you call a deer with no eyes\n",
      "\n",
      "EY\n",
      "________________________________________\n",
      "what do you call a deer with no eyes\n",
      "\n",
      "EYE why did the suntina fall off the middle to have a poo butt butt for dubly with a cow with a hants \n",
      "()\n",
      "Tweet no. 010\n",
      "=============\n",
      "Generating with seed:\n",
      "what did a bee say to a other bee?\n",
      "i lov\n",
      "________________________________________\n",
      "what did a bee say to a other bee?\n",
      "i love you and what do you call a man with a camer of the cow with a col in a banana with no eyes\n",
      "\n",
      " ​what\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "def sample(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 0.2\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "def generate_tweets(model, corpus, char_to_idx, idx_to_char, n_tweets=10, verbose=0): \n",
    "    model.load_weights('weights_seq40_step3.hdf5')\n",
    "    tweets = []\n",
    "    #spaces_in_corpus = np.array([idx for idx in range(CORPUS_LENGTH) if corpus[idx] == ' '])\n",
    "    for i in range(1, n_tweets + 1):\n",
    "        sequence = np.random.choice(jokes)[0:MAX_SEQ_LENGTH]\n",
    "        #begin = np.random.choice(spaces_in_corpus)\n",
    "        tweet = u''\n",
    "        #sequence = corpus[begin:begin + MAX_SEQ_LENGTH]\n",
    "        tweet += sequence\n",
    "        if verbose:\n",
    "            print('Tweet no. %03d' % i)\n",
    "            print('=' * 13)\n",
    "            print('Generating with seed:')\n",
    "            print(sequence)\n",
    "            print('_' * len(sequence))\n",
    "        for _ in range(100):\n",
    "            x = np.zeros((1, MAX_SEQ_LENGTH, N_CHARS))\n",
    "            for t, char in enumerate(sequence):\n",
    "                x[0, t, char_to_idx[char]] = 1.0\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_idx = sample(preds)\n",
    "            next_char = idx_to_char[next_idx]\n",
    "\n",
    "            tweet += next_char\n",
    "            sequence = sequence[1:] + next_char\n",
    "        if verbose:\n",
    "            print(tweet)\n",
    "            print()\n",
    "        tweets.append(tweet)\n",
    "    return tweets\n",
    "\n",
    "tweets = generate_tweets(model, corpus, char_to_idx, idx_to_char, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'how do dogs liveby eating food'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(jokes)[0:MAX_SEQ_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.422718712303\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(sequences)\n",
    "Xval = vectorizer.transform(tweets)\n",
    "print(pairwise_distances(Xval, Y=tfidf, metric='cosine').min(axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
